{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Multi-Layered Perceptrons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biological neurons are non-linear and are complex intricate networks. In machine learning, single neurons can only construct linear functions and decision boundaries. Single linear neurons can't solve XOR problems but one hidden layered MLPs can. One hidden layer MLPs can solve pratically any problem (given enough neurons in the hidden layer)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Function Approximation Theorem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Universal Function Approximation Theorem states that feedforward neural networks with single hidden layers containing finite numbers of neurons can approximate any continuous function on a compact input domain to arbitrary accuracy given a sufficiently large number of hidden neurons and appropriate choice of activiation function.\n",
    "\n",
    "This does beg the question of figuring out how many neurons to use. This theorem does not tell us how many hidden neurons we would need, all it's saying is that it is possible. It also doesn't tell us how to find these functions. There's no gurantee that we'll be able to find a function given a finite set of example input output pairs.\n",
    "\n",
    "To gain some more intuition behind it, let's think about what a ReLU function is. Essentially, rectified linear units or ReLU is an activation function that introduces non-linearity to a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x):\n",
    "\treturn max(0.0, x)\n",
    "\n",
    "reluBase = []\n",
    "for i in range(-2, 2):\n",
    "    reluBase.append(relu(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLu function takes the following shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reluBase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some modified ReLU functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_add_1(x):\n",
    "\treturn relu(x + 1)\n",
    "\n",
    "def relu_x_minus_two(x):\n",
    "\treturn -2 * max(0.0, x)\n",
    "\n",
    "def relu_minus_1(x):\n",
    "\treturn relu(x-1)\n",
    "\n",
    "relu1 = []\n",
    "relu2 = []\n",
    "relu3 = []\n",
    "\n",
    "Relu_Range = list(range(-3, 3))\n",
    "\n",
    "for i in Relu_Range:\n",
    "\trelu1.append(relu_add_1(i))\n",
    "\trelu2.append(relu_x_minus_two(i))\n",
    "\trelu3.append(relu_minus_1(i))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(Relu_Range, relu1)\n",
    "plt.title('relu(x + 1)')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(Relu_Range, relu2)\n",
    "plt.title('-2 * relu(x)')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(Relu_Range, relu3)\n",
    "plt.title('relu(x - 1)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then when we combine these, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_relu(x):\n",
    "    return relu_add_1(x) + relu_x_minus_two(x) + relu_minus_1(x)\n",
    "\n",
    "relu4 = []\n",
    "\n",
    "for i in list(range(-3, 3)):\n",
    "\trelu4.append(comb_relu(i))\n",
    "\n",
    "plt.plot(Relu_Range, relu4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that you've made a bit of a curve with a combination of ReLUs. Continuously doing this, we'll be able to replicate sin functions and create entire waves just using ReLUs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building MLPs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayered perceptrons are just simple linear layers stacked. The layers have activation functions or non-linearity functions labeled by $\\sigma$.\n",
    "\n",
    "For multi-class classification of `n` samples and `c` classes, we'd want to use the following loss function:\n",
    "- **Cross Entropy Loss**: Compares predicted class probability to actual class desired output and then penalizes probability based on how far it is from the actual expected value. This penalty is logarithmic and yields a large score for large differences close to 1 and small score for small differences tending to 0.\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname{loss}(x_i, \\text { labels }_i)=-\\log \\left(\\frac{\\exp (x[\\text { labels }_i])}{\\sum_{j} \\exp (x[j])}\\right)=-x_i[\\text { labels }_i]+\\log \\left(\\sum_{j=1}^C \\exp (x_i[j])\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a one-hidden-layer MLP with h hidden units, d inputs (features), we have a hidden layer denoted by $\\bold{H} \\in \\mathbb{R} ^ {n \\times h}$. Since the hidden and output layers are fully connected, the hidden-layer weights will be $\\bold{W}^{(1)} \\in \\mathbb{R} ^ {d \\times h}$ and biases $\\bold{b}^{(1)} \\in \\mathbb{R} ^ {1 \\times h}$. For the output-layer, we'll have weights $\\bold{W}^{(2)} \\in \\mathbb{R} ^ {h \\times q}$ and biases $\\bold{b}^{(2)} \\in \\mathbb{R} ^ {1 \\times q}$. This allows us to calculate the outputs $\\bold{O} \\in \\mathbb{R} ^ {n \\times q}$ as such:\n",
    "\n",
    "\n",
    "$$\n",
    "\\bold{H} = \\bold{XW}^{(1)} + b^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bold{O} = \\bold{HW}^{(2)} + b^{(2)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the activation functions, this becomes as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\bold{H} = \\sigma ( \\bold{XW}^{(1)} + b^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bold{O} = \\bold{HW}^{(2)} + b^{(2)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a very simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "batch_size = 5\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./Datasets', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./Datasets', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(32 * 32 * 3, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "  \n",
    "mlp = MLP()\n",
    "\n",
    "mlp.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "  running_loss = 0.0 \n",
    "\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp(inputs)\n",
    "    loss = loss_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "      print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "      running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = mlp(images.to(device))\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've discussed the ReLU activation function but there exist other options that we can use for activation functions. Different activation functions exist for different use cases. Each activation function also has it's own derivative which is also plotted. For the following, we make use PyTorch's built in activation functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-8.0, 8.0, 1, requires_grad=True)\n",
    "y = torch.relu(x)\n",
    "plt.plot(x.detach(), y.detach())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then from there, we're able to derivate the ReLU function as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(x), retain_graph=True)\n",
    "plt.plot(x.detach(), x.grad)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-8.0, 8.0, 1, requires_grad=True)\n",
    "y = torch.sigmoid(x)\n",
    "plt.plot(x.detach(), y.detach())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(x), retain_graph=True)\n",
    "plt.plot(x.detach(), x.grad)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-8.0, 8.0, 1, requires_grad=True)\n",
    "y =  torch.tanh(x)\n",
    "plt.plot(x.detach(), y.detach())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(x), retain_graph=True)\n",
    "plt.plot(x.detach(), x.grad)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
