{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Multi-Layered Perceptrons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biological neurons are non-linear and are complex intricate networks. In machine learning, single neurons can only construct linear functions and decision boundaries. Single linear neurons can't solve XOR problems but one hidden layered MLPs can. One hidden layer MLPs can solve pratically any problem (given enough neurons in the hidden layer)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Function Approximation Theorem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Universal Function Approximation Theorem states that feedforward neural networks with single hidden layers containing finite numbers of neurons can approximate any continuous function on a compact input domain to arbitrary accuracy given a sufficiently large number of hidden neurons and appropriate choice of activiation function.\n",
    "\n",
    "This does beg the question of figuring out how many neurons to use. This theorem does not tell us how many hidden neurons we would need, all it's saying is that it is possible. It also doesn't tell us how to find these functions. There's no gurantee that we'll be able to find a function given a finite set of example input output pairs.\n",
    "\n",
    "To gain some more intuition behind it, let's think about what a ReLU function is. Essentially, rectified linear units or ReLU is an activation function that introduces non-linearity to a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x):\n",
    "\treturn max(0.0, x)\n",
    "\n",
    "reluBase = []\n",
    "for i in range(-2, 2):\n",
    "    reluBase.append(relu(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLu function takes the following shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reluBase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some modified ReLU functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_add_1(x):\n",
    "\treturn relu(x + 1)\n",
    "\n",
    "def relu_x_minus_two(x):\n",
    "\treturn -2 * max(0.0, x)\n",
    "\n",
    "def relu_minus_1(x):\n",
    "\treturn relu(x-1)\n",
    "\n",
    "relu1 = []\n",
    "relu2 = []\n",
    "relu3 = []\n",
    "\n",
    "Relu_Range = list(range(-3, 3))\n",
    "\n",
    "for i in Relu_Range:\n",
    "\trelu1.append(relu_add_1(i))\n",
    "\trelu2.append(relu_x_minus_two(i))\n",
    "\trelu3.append(relu_minus_1(i))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(Relu_Range, relu1)\n",
    "plt.title('relu(x + 1)')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(Relu_Range, relu2)\n",
    "plt.title('-2 * relu(x)')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(Relu_Range, relu3)\n",
    "plt.title('relu(x - 1)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then when we combine these, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_relu(x):\n",
    "    return relu_add_1(x) + relu_x_minus_two(x) + relu_minus_1(x)\n",
    "\n",
    "relu4 = []\n",
    "\n",
    "for i in list(range(-3, 3)):\n",
    "\trelu4.append(comb_relu(i))\n",
    "\n",
    "plt.plot(Relu_Range, relu4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that you've made a bit of a curve with a combination of ReLUs. Continuously doing this, we'll be able to replicate sin functions and create entire waves just using ReLUs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building MLPs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayered perceptrons are just simple linear layers stacked. The layers have activation functions or non-linearity functions labeled by $\\sigma$.\n",
    "\n",
    "For multi-class classification of `n` samples and `c` classes, we'd want to use the following loss function:\n",
    "- **Cross Entropy Loss**: Compares predicted class probability to actual class desired output and then penalizes probability based on how far it is from the actual expected value. This penalty is logarithmic and yields a large score for large differences close to 1 and small score for small differences tending to 0.\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname{loss}(x_i, \\text { labels }_i)=-\\log \\left(\\frac{\\exp (x[\\text { labels }_i])}{\\sum_{j} \\exp (x[j])}\\right)=-x_i[\\text { labels }_i]+\\log \\left(\\sum_{j=1}^C \\exp (x_i[j])\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a one-hidden-layer MLP with h hidden units, d inputs (features), we have a hidden layer denoted by $\\bold{H} \\in \\mathbb{R} ^ {n \\times h}$. Since the hidden and output layers are fully connected, the hidden-layer weights will be $\\bold{W}^{(1)} \\in \\mathbb{R} ^ {d \\times h}$ and biases $\\bold{b}^{(1)} \\in \\mathbb{R} ^ {1 \\times h}$. For the output-layer, we'll have weights $\\bold{W}^{(2)} \\in \\mathbb{R} ^ {h \\times q}$ and biases $\\bold{b}^{(2)} \\in \\mathbb{R} ^ {1 \\times q}$. This allows us to calculate the outputs $\\bold{O} \\in \\mathbb{R} ^ {n \\times q}$ as such:\n",
    "\n",
    "\n",
    "$$\n",
    "\\bold{H} = \\bold{XW}^{(1)} + b^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bold{O} = \\bold{HW}^{(2)} + b^{(2)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the activation functions, this becomes as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\bold{H} = \\sigma ( \\bold{XW}^{(1)} + b^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bold{O} = \\bold{HW}^{(2)} + b^{(2)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a very simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "batch_size = 5\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./Datasets', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./Datasets', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(32 * 32 * 3, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "  \n",
    "mlp = MLP()\n",
    "\n",
    "mlp.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "  running_loss = 0.0 \n",
    "\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp(inputs)\n",
    "    loss = loss_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "      print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "      running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = mlp(images.to(device))\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've discussed the ReLU activation function but there exist other options that we can use for activation functions. Different activation functions exist for different use cases. Each activation function also has it's own derivative which is also plotted. For the following, we make use PyTorch's built in activation functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLU function, or the rectified linear unit provides a very simple nonlinear transformation. This algorithm only retains positive elemetns and discards all negative ones to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-8.0, 8.0, 1, requires_grad=True)\n",
    "y = torch.relu(x)\n",
    "plt.plot(x.detach(), y.detach())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then from there, we're able to derivate the ReLU function as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(x), retain_graph=True)\n",
    "plt.plot(x.detach(), x.grad)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function transforms its inputs to outputs that all lie on the interval (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-8.0, 8.0, 1, requires_grad=True)\n",
    "y = torch.sigmoid(x)\n",
    "plt.plot(x.detach(), y.detach())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the sigmoid function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(x), retain_graph=True)\n",
    "plt.plot(x.detach(), x.grad)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tanh or hyperbolic tangent function squashes inputs, transforming them into elements on the interval between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-8.0, 8.0, 1, requires_grad=True)\n",
    "y =  torch.tanh(x)\n",
    "plt.plot(x.detach(), y.detach())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the tanh function is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(x), retain_graph=True)\n",
    "plt.plot(x.detach(), x.grad)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Based Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between linear models and neural networks, the largest difference is the presence of nonlinearity of a neural network which causes the loss function to be non-convex. Neural networks are trained iteratively through gradient-based optimizers that slowly iterate towards the true function. We've been taking about gradient based learning throughout and we mention it again as it super important for neural networks.\n",
    "\n",
    "For feedforward neural networks, we need to ensure we're initializing the weights to some small random values. For the biases, it doesn't matter as much and they can be initialized to either zeroes or to small positive values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost functions are super important to consider. For most cases, parameteric models define a distribution of $p(\\bold{y} | \\bold{x}:\\bold{\\theta})$ and we can simply apply principles of maximum likelihood which means the cross-entropy between the training data and the model's predictions work as the cost function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most modern neural networks are trained using maximum likelihood which means the cost function is the negative log-likehood function which is also described as the cross-entropy between the training data and the model distribution. This cost function is given by:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\theta) = - \\mathbb{E}_{\\textbf{x,y} ~\\hat{p}_{data}} \\log p_{model}(\\textbf{y} | \\textbf{x})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific form of the cost function changes from model to model, depending on the specific form of $\\log{p_{model}}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to designing cost functions, we ened to be careful that they are large and predictable enough to guide the learning algorithm forwards. Functions that become flat overtime actually undermine this objective since they end up making the gradient very small.\n",
    "\n",
    "The choice of cost function is tightly coupled with the choice of output units as we often use the cross-entropy between the data distribution and the model distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple kind of output unit is the linear unit which is based on affine transformations with no nonlinearity. Linear units are often used to produce means of conditional Gaussian distributions. Linear units do not saturate which means they pose little difficulity for gradient based optimization algorithms.\n",
    "\n",
    "When we have tasks that require predicting the value of a binary variable $y$, the classification problems can be split into two classes. First is the maximum-likelihood approach which is to define a Bernoulli distribution over y conditioned on x which is defined by just a single number and the neural net only needs to predict $P(y=1|x)$. This number needs to lie in the interval [0,1]. To best do this, we can use a sigmoid output unit which uses a linear layer to first compute a linear solution which is then followed up with a sigmoid activation function to convert it into a probability. The sigmoid acivation function saturates to zero when the input becomes very negative and saturates to 1 when it becomes very positive. The gradient can shrink too small to be useful for learning in both cases and for this reason, maximum liklihood is typically the preferred approach to training sigmoid output units.\n",
    "\n",
    "When we want to represent a probability distribution over a discrete variable with n possible values, we can use the softmax function. This can be seen as sort of a generalization of the sigmoid function to represent a probability distribution over a binary variable. These functions are mostly seen as outputs of a classifier to represent the probability distribution over n different classes. Many objective functions other than the log-likelihood function do not work well with the softmax function. This specifically applies to objective functions that don't use a log to undo the exp of the softmax as they fail to learn when the argument of the exp becomes very negative, causing the gradient to vanish."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
