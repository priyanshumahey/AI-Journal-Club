{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is defined to be any modification we make to the learning algorithm to reduce generalization error but not the training error. This allows us to ensure the neural network works for new data beyond the training set. Many regularization approaches are based on limiting the capacity of models. For example, this could be adding a parameter norm penalty $\\Omega (\\bold{\\theta})$ to the objective function J. We denote the regularized objective function by $\\tilde{J}$:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{J} (\\theta ; \\bold{X}, \\bold{y}) = J(\\theta ; \\bold{X}, \\bold{y}) + \\alpha \\Omega(\\sigma)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\alpha \\in [0, \\infty]$ is a hyperparameter weighing the relative contribution of the norm penalty term $\\Omega$ relative to the standard objective function J. Setting $\\alpha$ to 0 results in no regularization whereas larger values mean more regularization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to make a machine learning model generalize better is to train it on more data. With dataset augmentation, we can modify or even add more data to provide the deep learning model with better resources to train. We could also potentially inject noise into the training data to help it generalize better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some models, the addition of noise is equivalent to imposing a penalty on the norm of the weights. Noise injection cn be much more powerful than simply shrinking the parameters, especially when noise is added to hidden units. Noise can be also implemented into the weights, which is something we'll be able to witness in the context of RNNs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With large models, after a while we gone through enough training data to sufficiently represent the dataset and using more can actually lead to overfitting. This leads to training error decreasing steadily over time but validation set error will continue to rise. This means we can obtain a model with better validation set error by returning the parameter setting to the point with the lowest validation set error. This is what **early stopping** does."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Representations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While weight decay acts to place a penalty directly on the model parameters, we can also place a penalty on the activations of the units in a neural network in an attempt to ake their activations sparse. This indirectly imposes a complicated penalty on the model parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond just making neural networks, it's important to also know how to optimize the model. Optimization means many things and includes not only increasing the speed at which the network learns but also the hardware side of things and making sure the neural networks you're training require less intense hardware."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization of Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to otpimizing the loss function, we need to remember that random search is very costly and we should carefully use gradient descent to optimize. \n",
    "\n",
    "When it comes to minimizing loss, how do we do it? Well to start off, loss can only really move left or right on the loss graph. We simply want to move the direction that makes the loss function. This simple principle helps us move further towards the minima."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent (SGD) and related are the most commonly used optimization algorithms for ML. A crucial parameter for SGD is learning rate ($\\epsilon$). In pratice, we decrease the learning rate over time. The learning rate can be choosen via trial and error but we can also choose it by observing learning curves and plotting cost functions over time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD can often be slow and momentum is designed to accelerate learning, espeically in the face of high curvature, small but consistent gradients or noisy gradients. Momentum accumulates an exponentially decaying moving average of past gradients and continues to move them in their direction.\n",
    "\n",
    "A hyperparameter $\\alpha \\in [0,1)$ determines the speed of the contributions of previous gradients exponentially decaying. The update rule is given by:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\bold{v} \\leftarrow \\alpha \\bold{v} - \\epsilon \\nabla_{\\theta} ( \\frac{1}{m} \\sum_{i=1}^{m} L(\\bold{f}(\\bold{x}^(i);\\theta), \\bold{y}^(i)))\n",
    "\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\theta \\leftarrow \\theta + v\n",
    "\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One variant of the momentum algorithm is Nesterov momentum. This is a small modification to the momentum algorithm that often leads to better performance. The update rule is given by:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v \\leftarrow \\alpha v - \\epsilon \\nabla_{\\theta} ( \\frac{1}{m} \\sum_{i=1}^{m} L(\\theta + \\alpha v, \\bold{y}^(i)))\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta \\leftarrow \\theta + v\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Convexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are known for being non-convex. This means that there are many local minima and saddle points. This means that the optimization algorithm can get stuck in a local minima and not be able to find the global minima. This is why we need to be careful when choosing the optimization algorithm.\n",
    "\n",
    "The non-convex nature of the surface of optimizating the cost function can lead to the optimization process being stuck in undesirable local-optima. This is why we need to be careful when choosing the optimization algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms with Adaptive Learning Rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate is a hyperparameter that is often hard to tune. Adaptive learning rate algorithms attempt to automatically tune the learning rate. These algorithms are often more robust to the choice of hyperparameters and can often be used to train models using the default learning rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AdaGrad algorithm adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For each parameter $\\theta_i$, AdaGrad keeps track of the sum of the squares of the gradients $\\bold{g}_i$ with respect to $\\theta_i$ up to time step t. AdaGrad performs well for some but not all deep learning models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorimth makes Adagrad better in non-convex settings by changing the gradient accumulation into an exponentially weighted moving average. This allows the algorithm to discard history from the extreme past so that it can converge rapidly after finding a convex bowl but continue to make progress towards a more optimal setting. RMSProp is usually a good choice and has been proven to be effective and practical."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSprop optimizer can be mathemetically described as such:\n",
    "\n",
    "$$\n",
    "v_t = \\alpha v_{t-1} + (1 - \\alpha) \\nabla_{\\theta} J(\\theta)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} \\nabla_{\\theta} J(\\theta)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam is another adaptive learning rate algorithm that is widely used. It combines the ideas of momentum and RMSProp. Adam is usually the best overall choice. Here, momentum is incorportated directly as an estimate of the first-order moment (with exponential weighting) of the gradient. The most straightforward way to add momentum to RMSProp is to apply momentum to the rescaled gradients. The use of momentum in combination with rescaling does not have a clear theoretical motivation. Adam includes bias corrections of both the first order moments and the second-order moments to account for initialization at the origin. Adam is usually the best overall choice."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
