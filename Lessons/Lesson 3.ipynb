{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Multi-Layered Perceptrons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biological neurons are non-linear and are complex intricate networks. In machine learning, single neurons can only construct linear functions and decision boundaries. Single linear neurons can't solve XOR problems but one hidden layered MLPs can. One hidden layer MLPs can solve pratically any problem (given enough neurons in the hidden layer)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Function Approximation Theorem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Universal Function Approximation Theorem states that feedforward neural networks with single hidden layers containing finite numbers of neurons can approximate any continuous function on a compact input domain to arbitrary accuracy given a sufficiently large number of hidden neurons and appropriate choice of activiation function.\n",
    "\n",
    "This does beg the question of figuring out how many neurons to use. This theorem does not tell us how many hidden neurons we would need, all it's saying is that it is possible. It also doesn't tell us how to find these functions. There's no gurantee that we'll be able to find a function given a finite set of example input output pairs.\n",
    "\n",
    "To gain some more intuition behind it, let's think about what a ReLU function is. Essentially, rectified linear units or ReLU is an activation function that introduces non-linearity to a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x):\n",
    "\treturn max(0.0, x)\n",
    "\n",
    "reluBase = []\n",
    "for i in range(-2, 2):\n",
    "    reluBase.append(relu(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLu function takes the following shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reluBase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some modified ReLU functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_add_1(x):\n",
    "\treturn relu(x + 1)\n",
    "\n",
    "def relu_x_minus_two(x):\n",
    "\treturn -2 * max(0.0, x)\n",
    "\n",
    "def relu_minus_1(x):\n",
    "\treturn relu(x-1)\n",
    "\n",
    "relu1 = []\n",
    "relu2 = []\n",
    "relu3 = []\n",
    "\n",
    "Relu_Range = list(range(-3, 3))\n",
    "\n",
    "for i in Relu_Range:\n",
    "\trelu1.append(relu_add_1(i))\n",
    "\trelu2.append(relu_x_minus_two(i))\n",
    "\trelu3.append(relu_minus_1(i))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(Relu_Range, relu1)\n",
    "plt.title('relu(x + 1)')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(Relu_Range, relu2)\n",
    "plt.title('-2 * relu(x)')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(Relu_Range, relu3)\n",
    "plt.title('relu(x - 1)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then when we combine these, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_relu(x):\n",
    "    return relu_add_1(x) + relu_x_minus_two(x) + relu_minus_1(x)\n",
    "\n",
    "relu4 = []\n",
    "\n",
    "for i in list(range(-3, 3)):\n",
    "\trelu4.append(comb_relu(i))\n",
    "\n",
    "plt.plot(Relu_Range, relu4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that you've made a bit of a curve with a combination of ReLUs. Continuously doing this, we'll be able to replicate sin functions and create entire waves just using ReLUs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building MLPs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayered perceptrons are just simple linear layers stacked. The layers have activation functions or non-linearity functions labeled by $\\sigma$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
