{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond just making neural networks, it's important to also know how to optimize the model. Optimization means many things and includes not only increasing the speed at which the network learns but also the hardware side of things and making sure the neural networks you're training require less intense hardware."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization of Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to otpimizing the loss function, we need to remember that random search is very costly and we should carefully use gradient descent to optimize. \n",
    "\n",
    "When it comes to minimizing loss, how do we do it? Well to start off, loss can only really move left or right on the loss graph. We simply want to move the direction that makes the loss function. This simple principle helps us move further towards the minima."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
