{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Machine Learning Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, one of the major goals is to uncover patterns hidden inside large datasets. But when doing this, it is important to ensure the pattern is generalizable which means that it applies to data beyond the dataset we've given to the algorithm for training.\n",
    "When we make AI, we want it to be able to generalize and figure out larger overall patterns. When using machine learning algorithms, it is highly important to make sure that the algorithm is able to discover a generalizable pattern.\n",
    "\n",
    "The phenomenon of fitting closer to our training data than to the underlying distribution is called _overfitting_, and techniques for combatting overfitting are called _regularization_ methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Error and Generalization Error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, for supervised learning settings, we assume the training data and the test data are drawn independently from identical distributions (IID assumption). Without this assumption, our models do not work. If this assumption isn't true, why would we beleive that one training distribution can tell us about another training distribution? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training error** $R_{emp}$ is a statistic calculated on the training dataset. **Generalization error** $R$, which is an expectation taken with respect to the underlying distribution. Generalization error can be thought of as what you would see if you applied your model to an infinite stream of additional data examples drawn from the same underlying data distribution. Formally, we can express training error as a sum:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R_{emp}[X,y,f] = \\frac{1}{n} \\sum_{i=1}^{n}l(x^{(i)}, y^{(i)}, f(x^{(i)}))\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization error can be expressed as a sum:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "R[p,f] = E_{(x,y)~P}[l(x,y,f(x))] = \\int \\space \\int l(x, y, f(x))p(x,y)dx dy\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can never truly calculate the generalization error exactly since we don't know the percise form of the density function and we cannot sample an infinite stream of datapoints. Thus, we estimate the generalization error by applying our model to an independent test set of randomly selected examples $X'$ and labels $y'$ that were withheld from our training set. This consists of applying the same formula as for calculating the empirical training error but to a test set $X'y'$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Complexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have simple models and abundant data, the training and generalization errors tend to be close. When we work with more complex models and/or fewer examples, we expect the training error to go down but the generalization gap continues to grow. \n",
    "\n",
    "Note that low training error does not necessariily imply low generalization error. To get a proper sense of the true error, we use a validation set and recieve a validation error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to training and validation erros, we need to be careful of two scenarios:\n",
    "\n",
    "- First is what happens bwhen our training error and validation error are both substantial but little gap between them. If the model is unable to reduce the training error, it could mean the model isn't expressive enough to capture the pattern (ie it's too simple). Since the generalization gap ($R_{emp} - R$) is small, we could use a more complex model. This is called **underfitting**.\n",
    "\n",
    "- If our training error is significantly lower than our validation error, this indicates our model is **overfitting**. This is due to the model recognizing too strong of a pattern on the training data and the training data only leading it to be poorly generalizable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Free Lunch Theorem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **no free lunch theorem** suggests that on averaging over on all possible data generating distributions, every classification has the same error rate when classifying previously unobserved points which means that no machine learning algorithm is universally better than others. This theorem suggests that the performance of all optimization algorithms are identical under some constraints. This theorem suggests we must design our machine learning algorithms to perform well on specific tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is an important part of tuning a model and helps to reduce overfitting. There are a couple of different regularization techniques which include familiar terms such as l1 and regularization.\n",
    "\n",
    "- **L1 regularization**: adds L1 penalty that is equal to the absolute value of the magnitude of the coefficient or simply restricting size of coefficients (eg Lasso regression)\n",
    "- **L2 Regularization**: adds L2 penalty that is equal to the square of the magnitude of coefficients (eg Ridge regression, SVM)\n",
    "- **Elastic Net**: L1 and L2 regularization combined together adding a hyperparamter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 regularization makes some coefficients zero meaning the model will ignore those features and this helps to emphasize the model's essential features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L1 = \\text{Loss Function} + \\lambda \\sum_{j=1}^{m} |w_j|\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\lambda$ controls the strength of regularization and $w_j$ represents the model's weights (coefficients)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regressions using L2 regression is called ridge regression and in this case, the cost function keeps the magnitude of the models weights as small as possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L2 = \\text{Loss Function} +\\frac{1}{2} \\lambda \\sum_{j=1}^{m} |w^2_j|\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\lambda$ controls the strength of regularization and $w_j$ represents the model's weights (coefficients)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic net is a regularization technique that combines ridge and lasso regularization and combines the penalities of lasso and ridge regression. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Elastic Net Cost Function} = \\text{Loss Function} + r \\lambda \\sum_{j=1}^m |wj|+ \\dfrac{(1-r)}{2} \\lambda\\sum{j=1}^m w_j^2$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Decay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight decay** is a regularization technique. It works by restricting the values that the paramters can take. Weight decay is a tool to penalize complexity and helps to add paramters to our loss function. Weight decay helps to minimize a loss function compromising the primary loss function and a penalty on the $L_2$ norm of the weights:\n",
    "\n",
    "$$\n",
    "L_{new}(w) = L_{original}(w) + \\lambda w^T w\n",
    "$$\n",
    "\n",
    "where $\\lambda$ determines the strength of the penalty, encouraging smaller weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Neural Networks for Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification problems put aside how much and focus instead on which category. Typically, classification is used to describe two different problems: (i) those where we are interested only in hard assignments of examples to classes and those where we wish to make soft assignments to assess probability that each category applies. In general, classification problems tend to not come with any sort of natural orderings among their classes. For representing categorical data, we can use **one-hot encoding** which is when we use a vector with as many components as we have categories and tehn each component corresponds to a particular instance. For example, if dog, cat and bird can be represented with (1,0,0), (0,1,0) and (0,0,1) respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate probabilities associated with all possible classes, we need a model with multiple outputs (one per class). Each output corresponds to its own affine function. Let's say we have 4 features and 3 possible outputs. We'll need 12 scalars to represent the weights and 3 scalars to represent the biases. For a simple neural network, this could yield:\n",
    "\n",
    "$$\n",
    "o_1 = x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_2 = x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_3 = x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouput layer depends on all the inputs, $x_1, x_2, x_3 \\text{ and } x_4$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important function you need is a function that turns a vector of K real values into a vector K real values that sum up to 1. The inputs can be positive, negative, or zero but all get transformed into values between 0 and 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bold{\\hat{y}} = \\text{softmax}(\\bold{o})\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\hat{y_i} = \\frac{\\text{exp}(o_i)}{\\sum _j \\text(exp)(o_j)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(vec):\n",
    "  exponential = np.exp(vec)\n",
    "  probabilities = exponential / np.sum(exponential)\n",
    "  return probabilities\n",
    "\n",
    "vector = np.array([1.0, 3.0, 2.0])\n",
    "probabilities = softmax(vector)\n",
    "print(\"Probability Distribution is:\")\n",
    "print(probabilities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're mapping from features x to probabilities $\\hat{y}$, we need a way to optimize the accuracy of this mapping. To do this, we'll rely on maximum likelihood estimation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-Likelihood\n",
    "\n",
    "The softmax function gives us a vector $\\hat{y}$ which we can interpret as conditional probabilities of each class. Assume that we have a dataset with features X and labels Y represented using a one-hot encoding label vector. We are able to compare our estimates with the truth values by checking how probable the actual classes are according to our model given the features:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(Y | X) = \\prod_{i=1}^{n}P(y^{(i)}|x^{(i)})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any pair of label y and prediction y over q classes, we get the following loss function:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "l(y, \\hat{y}) = - \\sum_{j=1}^{q}y_j \\log \\hat{y}_j\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss function is commonly called cross-entropy loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
